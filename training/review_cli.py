"""
Terminal-based human review tool for label verification.

Loads training/review-queue.json (generated by generate_review_queue.py)
and presents each entry for human review, prioritized by tier:
  Tier 0: DANGEROUS-labeled skills (verify these are truly dangerous)
  Tier 1: MLP/LLM disagreements (one of them is wrong)
  Tier 2: Short LLM reasoning (low-confidence labels)

Usage:
    python training/review_cli.py
    python training/review_cli.py --queue training/review-queue.json --limit 50
"""

import argparse
import json
import sys
from pathlib import Path

# Label colors for terminal
COLORS = {
    "SAFE": "\033[92m",       # green
    "CAUTION": "\033[93m",    # yellow
    "DANGEROUS": "\033[91m",  # red
    "RESET": "\033[0m",
    "BOLD": "\033[1m",
    "DIM": "\033[2m",
}


def colored(text: str, color: str) -> str:
    return f"{COLORS.get(color, '')}{text}{COLORS['RESET']}"


def load_review_queue(path: str) -> list[dict]:
    p = Path(path)
    if not p.exists():
        print(f"ERROR: Review queue not found at {path}", file=sys.stderr)
        print("Run: python training/generate_review_queue.py", file=sys.stderr)
        sys.exit(1)
    return json.loads(p.read_text())


def load_existing_overrides(path: str) -> dict[str, str]:
    p = Path(path)
    if not p.exists():
        return {}
    try:
        data = json.loads(p.read_text())
        if isinstance(data, list):
            return {e["skill_name"]: e["label"] for e in data if "skill_name" in e and "label" in e}
        return {}
    except (json.JSONDecodeError, KeyError):
        return {}


def save_overrides(overrides: dict[str, str], path: str):
    entries = [{"skill_name": name, "label": label} for name, label in sorted(overrides.items())]
    Path(path).write_text(json.dumps(entries, indent=2) + "\n")


def display_entry(entry: dict, index: int, total: int):
    tier = entry.get("review_tier", "STANDARD")
    skill_name = entry.get("skill_name", "unknown")
    llm_label = entry.get("llm_label", "?")
    model_pred = entry.get("model_prediction", "?")
    reasoning = entry.get("reasoning", "")
    model_scores = entry.get("model_scores", {})

    print(f"\n{'='*72}")
    print(f"  [{index+1}/{total}]  {colored(skill_name, 'BOLD')}  |  Tier: {tier}")
    print(f"{'='*72}")

    # Labels side by side
    llm_colored = colored(llm_label, llm_label)
    model_colored = colored(model_pred, model_pred) if model_pred else "N/A"
    disagree_marker = colored("  ** DISAGREE **", "DANGEROUS") if entry.get("disagreement") else ""
    print(f"  LLM label:   {llm_colored}")
    print(f"  MLP predict: {model_colored}{disagree_marker}")

    # Model scores
    if model_scores:
        scores_str = "  MLP scores:  "
        for cls in ("SAFE", "CAUTION", "DANGEROUS"):
            score = model_scores.get(cls, 0.0)
            if isinstance(score, (int, float)):
                scores_str += f"{cls}={score:.3f}  "
        print(scores_str)

    # Reasoning
    print(f"\n  {colored('LLM Reasoning:', 'DIM')}")
    if reasoning:
        # Word-wrap at ~68 chars
        words = reasoning.split()
        line = "    "
        for word in words:
            if len(line) + len(word) + 1 > 70:
                print(line)
                line = "    "
            line += word + " "
        if line.strip():
            print(line)
    else:
        print("    (no reasoning provided)")

    # Skill content preview
    skill_md = entry.get("skill_md", "")
    if skill_md:
        preview = skill_md[:500]
        if len(skill_md) > 500:
            preview += "..."
        print(f"\n  {colored('Skill content (first 500 chars):', 'DIM')}")
        for line in preview.split("\n")[:15]:
            print(f"    {line}")
        if len(preview.split("\n")) > 15:
            print(f"    ...")


def prompt_label() -> str | None:
    """Prompt user for label decision.

    Returns: 'SAFE', 'CAUTION', 'DANGEROUS', 'keep', or 'skip'.
    """
    print(f"\n  {colored('[S]', 'SAFE')}AFE  "
          f"{colored('[C]', 'CAUTION')}AUTION  "
          f"{colored('[D]', 'DANGEROUS')}ANGEROUS  "
          f"[K]eep  [s]kip  [q]uit")

    while True:
        try:
            choice = input("  Your label: ").strip().lower()
        except (EOFError, KeyboardInterrupt):
            return None

        if choice in ("s", "safe"):
            return "SAFE"
        elif choice in ("c", "caution"):
            return "CAUTION"
        elif choice in ("d", "dangerous"):
            return "DANGEROUS"
        elif choice in ("k", "keep"):
            return "keep"
        elif choice in ("skip", ""):
            return "skip"
        elif choice in ("q", "quit"):
            return None
        else:
            print("  Invalid input. Use S/C/D/K/skip/q")


def main():
    parser = argparse.ArgumentParser(description="Terminal-based label review tool")
    parser.add_argument(
        "--queue", type=str, default="training/review-queue.json",
        help="Path to review queue (from generate_review_queue.py)",
    )
    parser.add_argument(
        "--output", type=str, default="training/human_review.json",
        help="Path to write label overrides",
    )
    parser.add_argument(
        "--limit", type=int, default=0,
        help="Max entries to review (0 = all)",
    )
    parser.add_argument(
        "--tier", type=int, default=None,
        help="Only show entries from this tier (0=DANGEROUS, 1=disagree, 2=short reasoning)",
    )
    parser.add_argument(
        "--labels", type=str, default="training/real-labels.json",
        help="Path to labeled dataset (for skill_md content lookup)",
    )
    args = parser.parse_args()

    # Load review queue
    queue = load_review_queue(args.queue)
    print(f"Loaded {len(queue)} entries from {args.queue}")

    # Optionally enrich with skill_md from labels file
    labels_path = Path(args.labels)
    skill_md_map = {}
    if labels_path.exists():
        try:
            labels_data = json.loads(labels_path.read_text())
            if isinstance(labels_data, list):
                skill_md_map = {e["skill_name"]: e.get("skill_md", "") for e in labels_data}
                print(f"Loaded skill content from {args.labels}")
        except (json.JSONDecodeError, KeyError):
            pass

    # Enrich queue entries with skill_md if missing
    for entry in queue:
        if not entry.get("skill_md") and entry.get("skill_name") in skill_md_map:
            entry["skill_md"] = skill_md_map[entry["skill_name"]]

    # Filter by tier if requested
    tier_names = ["DANGEROUS_LABEL", "MLP_LLM_DISAGREEMENT", "SHORT_REASONING", "STANDARD"]
    if args.tier is not None and 0 <= args.tier < len(tier_names):
        target_tier = tier_names[args.tier]
        queue = [e for e in queue if e.get("review_tier") == target_tier]
        print(f"Filtered to tier {args.tier} ({target_tier}): {len(queue)} entries")

    # Apply limit
    if args.limit > 0:
        queue = queue[:args.limit]

    # Load existing overrides
    overrides = load_existing_overrides(args.output)
    if overrides:
        print(f"Loaded {len(overrides)} existing overrides from {args.output}")

    # Show tier summary
    tier_counts = {}
    for entry in queue:
        t = entry.get("review_tier", "STANDARD")
        tier_counts[t] = tier_counts.get(t, 0) + 1
    print("\nReview queue summary:")
    for tier_name in tier_names:
        count = tier_counts.get(tier_name, 0)
        if count > 0:
            print(f"  {tier_name}: {count}")
    print(f"  Total: {len(queue)}")

    # Review loop
    reviewed = 0
    overridden = 0

    for i, entry in enumerate(queue):
        display_entry(entry, i, len(queue))
        label = prompt_label()

        if label is None:
            print("\n  Quitting review...")
            break

        if label == "skip":
            continue

        reviewed += 1
        skill_name = entry.get("skill_name", "")

        if label == "keep":
            # Remove any existing override for this skill
            if skill_name in overrides:
                del overrides[skill_name]
                print(f"  -> Removed override for {skill_name}")
        else:
            # Set override
            overrides[skill_name] = label
            overridden += 1
            llm_label = entry.get("llm_label", "?")
            if label != llm_label:
                print(f"  -> Override: {llm_label} -> {colored(label, label)}")
            else:
                print(f"  -> Confirmed: {colored(label, label)}")

        # Save after each review (in case of crash/quit)
        save_overrides(overrides, args.output)

    # Final summary
    print(f"\n{'='*72}")
    print(f"Review complete: {reviewed} reviewed, {overridden} overrides written")
    print(f"Overrides saved to {args.output}")
    print(f"Total overrides on file: {len(overrides)}")

    return 0


if __name__ == "__main__":
    sys.exit(main())
